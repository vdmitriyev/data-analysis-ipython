{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "Analysis of dataset provided within the competition called [ML Boot Camp III](http://mlbootcamp.ru/championship/10/) organized by MailRu Group in 2017.\n",
    "\n",
    "### Links\n",
    "\n",
    "* [ML Boot Camp](http://mlbootcamp.ru/championship/10/)\n",
    "\n",
    "\n",
    "### Description\n",
    "\n",
    "#### Task (in Russian)\n",
    "\n",
    "Задача называется \"Выход из он-лайн игры\". В этой задаче необходимо научиться предсказывать, остается ли участник в он-лайн игре или уходит из нее. Уходом считается отсутствие его в игре в течение недели.\n",
    "\n",
    "#### Dataset Information  (in Russian)\n",
    "\n",
    "Всего используется 12 признаков, вычисленных за 2 предыдущие недели:\n",
    "\n",
    "* maxPlayerLevel - максимальный уровень игры, который прошел игрок\n",
    "* numberOfAttemptedLevels - количество уровней, которые попытался пройти игрок\n",
    "* attemptsOnTheHighestLevel - число попыток, сделанных на самом высоком уровне\n",
    "* totalNumOfAttempts - общее число попыток\n",
    "* averageNumOfTurnsPerCompletedLevel - среднее количество ходов, выполненных на успешно пройденных уровнях\n",
    "* doReturnOnLowerLevels - делал ли игрок возвраты к игре на уже пройденных уровнях\n",
    "* numberOfBoostersUsed - количество использованных бустеров\n",
    "* fractionOfUsefullBoosters - количество бустеров, использованных во время успешных попыток (игрок прошел уровнь)\n",
    "* totalScore - общее количество набранных очков\n",
    "* totalBonusScore - общее количество набранных бонусных очков\n",
    "* totalStarsCount - общее количество набранных звезд\n",
    "* numberOfDaysActuallyPlayed - количество дней, когда пользователь играл в игру\n",
    "\n",
    "Все предоставленные для задачи данные разбиты на две части: обучающую (x_train.csv и y_train.csv) и тестовую (x_test.csv). Каждая строка файлов x_train.csv и x_test.csv соответствует одному пользователю. Данные в строке разделены точкой с запятой. Первая строка содержит имена признаков. Файл y_train.csv содержит значения 1 или 0 в зависимости от того, остался пользователь в игре или вышел из нее соответственно.\n",
    "\n",
    "Как обучающая (x_train.csv и y_train.csv), так и тестовая (x_test.csv) выборки содержат информацию о **25289** пользователях.\n",
    "\n",
    "### Submission (in Russian)\n",
    "\n",
    "В качестве ответа для данной задачи принимается текстовый файл, каждая строка которого соответствует строке в файле x_test.csv и содержит значение от 0 до 1 (вероятность того, что пользователь останется в игре). В качестве критерия качества решения задачи используется [логарифмическая функция потерь](https://www.kaggle.com/wiki/LogarithmicLoss).\n",
    "\n",
    "Количество посылок ограничено пятью в сутки.\n",
    "\n",
    "Тестовая выборка случайным образом разбита на две части в соотношении 40/60. Результат на первых 40% будет определять положение участников в рейтинговой таблице на всем протяжении конкурса. Результат на оставшихся 60% станет известен после окончания конкурса и именно он определит финальную расстановку участников.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data\n",
    "\n",
    "The first step is always a data loading. Before loading data was converted from original Excel file into CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.style.use('fivethirtyeight')\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "dataset = pd.read_csv('data/ENB2012-data-uci.csv', sep=';')\n",
    "columns = ['X1 Relative Compactness',\n",
    "           'X2 Surface Area',\n",
    "           'X3 Wall Area',\n",
    "           'X4 Roof Area',\n",
    "           'X5 Overall Height',\n",
    "           'X6 Orientation',\n",
    "           'X7 Glazing Area',\n",
    "           'X8 Glazing Area Distribution',\n",
    "           'y1 Heating Load',\n",
    "           'y2 Cooling Load']\n",
    "\n",
    "mapping = {'X1' : columns[0], \n",
    "           'X2' : columns[1],\n",
    "           'X3' : columns[2],\n",
    "           'X4' : columns[3],\n",
    "           'X5' : columns[4],\n",
    "           'X6' : columns[5],\n",
    "           'X7' : columns[6],\n",
    "           'X8' : columns[7],\n",
    "           'Y1' : columns[8],\n",
    "           'Y2' : columns[9]}\n",
    "\n",
    "mapping = collections.OrderedDict(sorted(mapping.items()))\n",
    "target_columns = [mapping[x] for x in mapping if x[0] != 'Y']\n",
    "#print (target_columns)\n",
    "\n",
    "dataset.columns = columns\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset[[mapping[\"X1\"], mapping[\"X2\"]]].hist(bins=10, figsize = (10,5))\n",
    "dataset[[mapping[\"X3\"], mapping[\"X4\"]]].hist(bins=10, figsize = (10,5))\n",
    "dataset[[mapping[\"X5\"], mapping[\"X6\"]]].hist(bins=10, figsize = (10,5))\n",
    "dataset[[mapping[\"X7\"], mapping[\"X8\"]]].hist(bins=10, figsize = (10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# try all styles\n",
    "# for style in plt.style.available:\n",
    "#     plt.style.use(style)\n",
    "#     print (style)\n",
    "#     dataset[[mapping[\"X1\"], mapping[\"X2\"]]].hist(bins=10, figsize = (10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# different view of histograms\n",
    "#dataset[[\"X1\", \"X2\", \"X3\", \"X4\", \"X5\", \"X6\", \"X7\", \"X8\"]].hist(bins=10, figsize = (10,10) , normed=1)\n",
    "dataset.hist(bins=10, figsize = (20,20), normed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting target/output variables in form of histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset[[mapping[\"Y1\"], mapping[\"Y2\"]]].hist(bins=10, figsize = (10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# normalized\n",
    "dataset[[mapping[\"Y1\"], mapping[\"Y2\"]]].hist(bins=10, figsize = (10,5), normed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalizing Data in Dataset - Multiple Approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize_sklearn(dataset):\n",
    "    \"\"\"normalization of an input dataset\"\"\"\n",
    "    from sklearn import preprocessing\n",
    "    x = dataset.values #returns a numpy array\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    df_norm = pd.DataFrame(x_scaled)\n",
    "    return df_norm\n",
    "\n",
    "def normalize_manually(df):\n",
    "    result = df.copy()\n",
    "    for feature_name in df.columns:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result\n",
    "\n",
    "df_norm_v1 = normalize_sklearn(dataset)\n",
    "df_norm_v2 = normalize_manually(dataset)\n",
    "df_norm = df_norm_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_norm_v1.head(5)\n",
    "#df_norm_v1.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_norm_v2.head(5)\n",
    "#df_norm_v2.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Normalization Approaches\n",
    "\n",
    "Replacing all values that are smaller then particular epsilon (my own choice) with zeros and counting total amount of zeros in each collumn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from pandas.util.testing import assert_frame_equal\n",
    "#assert_frame_equal(df_norm, df_norm_v2)\n",
    "\n",
    "def compare_normalization_approaches(df_norm_v1, df_norm_v2):\n",
    "    \"\"\"Replacing all values that are smaller then particular epsilon (my own choice) \n",
    "       with zeros and counting total amount of zeros in each collumn.\n",
    "    \"\"\"\n",
    "    EPSILON = 0.000000000000001\n",
    "    comparison_array = df_norm_v1.values - df_norm_v2.values\n",
    "    comparison_array[comparison_array < EPSILON] = 0\n",
    "    print ('Total number of zeros in earch row (in each should be 768)')\n",
    "    print ((comparison_array == 0).astype(int).sum(axis=0))\n",
    "    \n",
    "compare_normalization_approaches(df_norm_v1, df_norm_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df_norm_v1.hist(bins=10, figsize = (15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boxplot\n",
    "\n",
    "Making plots with boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_multiple_boxplots():\n",
    "    \n",
    "    for key in mapping:\n",
    "        if key[0] != 'Y':\n",
    "            dataset[[mapping[key]]].plot.box(figsize = (5,5))\n",
    "            \n",
    "# plot_multiple_boxplots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset[target_columns].plot.box(figsize = (15,10), vert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_norm[target_columns].plot.box(figsize = (15,10), vert=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter Plot\n",
    "\n",
    "Making scatter plot that compares normalized values of input with normalized values of output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "scatter_matrix(df_norm[target_columns], alpha=0.2, figsize=(25, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_scatter(df, target_output):\n",
    "    \"\"\" Making scatter plot for input/output comparison \"\"\"\n",
    "    \n",
    "    if target_output not in ('Y1', 'Y2'):\n",
    "        print ('Wrong target output variable for plotting')\n",
    "        return None\n",
    "    \n",
    "    for key in mapping:\n",
    "        if key[0] != 'Y':\n",
    "            df.plot.scatter(x=mapping[key], y=mapping[target_output], figsize = (3,3))\n",
    "\n",
    "#plot_scatter(df_norm, 'Y1') \n",
    "#plot_scatter(df_norm, 'Y2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing all input values with output **'y1 Heating Load'** and **'y2 Cooling Load'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in mapping:\n",
    "    \n",
    "    if key[0] != 'Y':\n",
    "        fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=False, figsize = (12,6))\n",
    "        df_norm.plot.scatter(x=mapping[key], y=mapping[\"Y1\"], ax=ax1, grid=True)\n",
    "        df_norm.plot.scatter(x=mapping[key], y=mapping[\"Y2\"], ax=ax2, grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations\n",
    "\n",
    "After plotting data in form of scatter and histogram, it's make sense to prove correlation between input variables and output onces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('{feature}\\t\\t\\t\\t{correlation_with_Y1}\\t{correlation_with_Y2}'.format(feature='Key', \n",
    "                                                                       correlation_with_Y1='correlation_with_Y1', \n",
    "                                                                       correlation_with_Y2='cor_spearman_y2'))\n",
    "for key in mapping:\n",
    "    if key[0] != 'Y':\n",
    "        cor_spearman_y1 = df_norm[mapping[key]].corr(df_norm[mapping[\"Y1\"]], method='spearman')\n",
    "        cor_spearman_y2 = df_norm[mapping[key]].corr(df_norm[mapping[\"Y2\"]], method='spearman')\n",
    "        print ('{feature}\\t\\t\\t{correlation_with_Y1}\\t\\t{correlation_with_Y2}'.format(feature=mapping[key], \n",
    "                                                                               correlation_with_Y1=cor_spearman_y1, \n",
    "                                                                               correlation_with_Y2=cor_spearman_y2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the correlation between input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_norm[target_columns].corr(method='spearman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Machine Learning Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def random_forest_dataset_pima_diabetes():\n",
    "    \"\"\" Sample usage of Random Forest taken from following URL:\n",
    "        - http://machinelearningmastery.com/ensemble-machine-learning-algorithms-python-scikit-learn/\n",
    "        \n",
    "        Dataset:\n",
    "            Number of Instances: 768\n",
    "\n",
    "            For Each Attribute: (all numeric-valued)\n",
    "               1. Number of times pregnant\n",
    "               2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n",
    "               3. Diastolic blood pressure (mm Hg)\n",
    "               4. Triceps skin fold thickness (mm)\n",
    "               5. 2-Hour serum insulin (mu U/ml)\n",
    "               6. Body mass index (weight in kg/(height in m)^2)\n",
    "               7. Diabetes pedigree function\n",
    "               8. Age (years)\n",
    "   \n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import sklearn\n",
    "    from sklearn import model_selection\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "    names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "    dataframe = pd.read_csv(url, names=names)\n",
    "    print(dataframe.describe())\n",
    "    \n",
    "#     # visualization\n",
    "#     import matplotlib.pyplot as plt\n",
    "#     plt.style.use('default')\n",
    "#     dataframe.hist(figsize = (15,10))\n",
    "    \n",
    "    array = dataframe.values\n",
    "    X = array[:, 0:8]\n",
    "    Y = array[:, 8]\n",
    "    seed = 7\n",
    "    num_trees = 100\n",
    "    max_features = 8\n",
    "    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n",
    "    model = RandomForestClassifier(n_estimators=num_trees, max_features=max_features)\n",
    "    results = model_selection.cross_val_score(model, X, Y, cv=kfold)\n",
    "    print('\\nRF results: {0}'.format(results.mean()))\n",
    "\n",
    "# for value in range(10):\n",
    "#     random_forest_dataset_pima_diabetes()\n",
    "\n",
    "random_forest_dataset_pima_diabetes()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
